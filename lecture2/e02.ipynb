{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c6cffb",
   "metadata": {},
   "source": [
    "### E02\n",
    "Split up the dataset randomly into 80% train set, 10% dev set, 10% test set. \n",
    "\n",
    "Train the bigram and trigram models only on the training set. \n",
    "\n",
    "Evaluate them on dev and test splits. What can you see?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ade770d",
   "metadata": {},
   "source": [
    "### Setting up datasets\n",
    "- Load all the names into the list\n",
    "- Split the list into 80% train set, 10% dev set, 10% test set\n",
    "- Form stoi and itos dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01f6ac7f",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 32033\n",
      "2 15\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset into a list\n",
    "with open('names.txt', 'r') as file:\n",
    "    names = file.read().split()\n",
    "\n",
    "print('Total names:', len(names))\n",
    "print(min(len(item) for item in names), max(len(item) for item in names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5eec7ec8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Splitting names into 80% train set, 10% dev set, 10% test set\n",
    "import random\n",
    "\n",
    "random.shuffle(names) # Shuffles the list in-place\n",
    "\n",
    "split_1, split_2 = int(0.8*len(names)), int(0.9*len(names))\n",
    "train_list = names[:split_1]\n",
    "dev_list = names[split_1:split_2]\n",
    "test_list = names[split_2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "27b60a29",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 79.998751 %\n",
      "Dev set: 9.999063 %\n",
      "Test set: 10.002185 %\n"
     ]
    }
   ],
   "source": [
    "print(f'Training set: {len(train_list)/len(names)*100.:4f} %')\n",
    "print(f'Dev set: {len(dev_list)/len(names)*100.:4f} %')\n",
    "print(f'Test set: {len(test_list)/len(names)*100.:4f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "87c8fca3",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Form stoi and itos\n",
    "vocab = sorted(list(set(''.join(names))))\n",
    "stoi = {s: i+1 for i, s in enumerate(vocab)}\n",
    "stoi['.'] = 0\n",
    "itos = {stoi[s]: s for s in stoi}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2798b08a",
   "metadata": {},
   "source": [
    "### Bigram: training\n",
    "- Perform Gradient descent and update weights only based on training list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "417d50a8",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Preparing bigrams\n",
    "xs, ys = [], []\n",
    "for name in train_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for xi, yi in zip(name, name[1:]):\n",
    "        xs.append(stoi[xi])\n",
    "        ys.append(stoi[yi])\n",
    "\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "\n",
    "# Encoding the inputs\n",
    "import torch.nn.functional as F\n",
    "xenc = F.one_hot(xs, num_classes=27).float()\n",
    "yenc = F.one_hot(ys, num_classes=27).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d519ea78",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Initialization\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W_bigram = torch.randn(27, 27, generator=g, requires_grad=True)\n",
    "logits = (xenc @ W_bigram)\n",
    "counts = logits.exp()\n",
    "probs = counts / counts.sum(1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a5112",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7582688331604004\n",
      "3.369582414627075\n",
      "3.1526994705200195\n",
      "3.0192978382110596\n",
      "2.926764488220215\n",
      "2.859539747238159\n",
      "2.8089253902435303\n",
      "2.7693326473236084\n",
      "2.737311363220215\n",
      "2.7107222080230713\n",
      "2.688204765319824\n",
      "2.6688623428344727\n",
      "2.652076482772827\n",
      "2.6373987197875977\n",
      "2.6244869232177734\n",
      "2.613070487976074\n",
      "2.60292649269104\n",
      "2.5938706398010254\n",
      "2.5857484340667725\n",
      "2.578429698944092\n",
      "2.571805715560913\n",
      "2.5657849311828613\n",
      "2.5602893829345703\n",
      "2.5552542209625244\n",
      "2.550623655319214\n",
      "2.5463504791259766\n",
      "2.54239559173584\n",
      "2.5387251377105713\n",
      "2.535308837890625\n",
      "2.532122850418091\n",
      "2.5291450023651123\n",
      "2.526357412338257\n",
      "2.52374267578125\n",
      "2.521287441253662\n",
      "2.518977165222168\n",
      "2.5168018341064453\n",
      "2.5147504806518555\n",
      "2.5128138065338135\n",
      "2.5109832286834717\n",
      "2.5092508792877197\n",
      "2.5076098442077637\n",
      "2.5060532093048096\n",
      "2.504575729370117\n",
      "2.503171682357788\n",
      "2.501835823059082\n",
      "2.500563859939575\n",
      "2.4993512630462646\n",
      "2.498194456100464\n",
      "2.4970896244049072\n",
      "2.4960334300994873\n",
      "2.495023488998413\n",
      "2.494056224822998\n",
      "2.4931297302246094\n",
      "2.4922404289245605\n",
      "2.491387128829956\n",
      "2.490567445755005\n",
      "2.4897797107696533\n",
      "2.4890217781066895\n",
      "2.4882922172546387\n",
      "2.4875895977020264\n",
      "2.4869120121002197\n",
      "2.4862587451934814\n",
      "2.485628128051758\n",
      "2.4850189685821533\n",
      "2.4844305515289307\n",
      "2.4838616847991943\n",
      "2.483311176300049\n",
      "2.482778549194336\n",
      "2.48226261138916\n",
      "2.4817628860473633\n",
      "2.481278657913208\n",
      "2.480809211730957\n",
      "2.480353355407715\n",
      "2.4799110889434814\n",
      "2.4794814586639404\n",
      "2.479064702987671\n",
      "2.478659152984619\n",
      "2.4782650470733643\n",
      "2.477882146835327\n",
      "2.4775092601776123\n",
      "2.477146863937378\n",
      "2.4767937660217285\n",
      "2.476449728012085\n",
      "2.4761149883270264\n",
      "2.4757888317108154\n",
      "2.475471019744873\n",
      "2.47516131401062\n",
      "2.4748589992523193\n",
      "2.47456431388855\n",
      "2.474276542663574\n",
      "2.473996162414551\n",
      "2.473722457885742\n",
      "2.473454713821411\n",
      "2.473193407058716\n",
      "2.472938299179077\n",
      "2.472688913345337\n",
      "2.472445249557495\n",
      "2.4722073078155518\n",
      "2.4719743728637695\n",
      "2.4717466831207275\n"
     ]
    }
   ],
   "source": [
    "num = ys.nelement() # number of examples\n",
    "\n",
    "# Gradient descent\n",
    "for _ in range(100):\n",
    "    # Forward pass\n",
    "    logits = xenc @ W_bigram # predicted log counts\n",
    "\n",
    "    # softmax (next two lines): converts logits to probabilities\n",
    "    counts = logits.exp() # predicted counts\n",
    "    probs = counts / counts.sum(1, keepdims=True) # calculating probablities\n",
    "    \n",
    "    loss = -probs[torch.arange(num), ys].log().mean()\n",
    "\n",
    "    print(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    W_bigram.grad = None\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the weights\n",
    "    W_bigram.data += -50 * W_bigram.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804ef347",
   "metadata": {},
   "source": [
    "### Bigram: loss on dev and test sets\n",
    "- Prepare xs & corresponding ys dataset on dev & test\n",
    "- Evaluate loss of Dev set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ab1e34e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "xs_dev, xs_test, ys_dev, ys_test = [], [], [], []\n",
    "for name in dev_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(name, name[1:]):\n",
    "        xs_dev.append(stoi[ch1])\n",
    "        ys_dev.append(stoi[ch2])\n",
    "\n",
    "for name in test_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2 in zip(name, name[1:]):\n",
    "        xs_test.append(stoi[ch1])\n",
    "        ys_test.append(stoi[ch2])\n",
    "\n",
    "xs_dev = torch.tensor(xs_dev)\n",
    "ys_dev = torch.tensor(ys_dev)\n",
    "xs_test = torch.tensor(xs_test)\n",
    "ys_test = torch.tensor(ys_test)\n",
    "\n",
    "num_dev = len(xs_dev)\n",
    "num_test = len(xs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0bb8cdd",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def compute_bigram_loss(xs, ys, W):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3053983b",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dev_loss = compute_bigram_loss(xs_dev, ys_dev, W_bigram)\n",
    "test_loss = compute_bigram_loss(xs_dev, ys_dev, W_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "725ac371",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4807989597320557 2.4807989597320557\n"
     ]
    }
   ],
   "source": [
    "print(dev_loss.item(), test_loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273c3705",
   "metadata": {},
   "source": [
    "### Trigrams: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb6bf16d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "xs_train, ys_train = [], []\n",
    "for name in train_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs_train.append([stoi[ch1], stoi[ch2]])\n",
    "        ys_train.append(stoi[ch3])\n",
    "\n",
    "xs_train = torch.tensor(xs_train)\n",
    "ys_train = torch.tensor(ys_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f84bed82",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# One hot encoding of the inputs\n",
    "import torch.nn.functional as F\n",
    "xenc_train = F.one_hot(xs_train, num_classes=27).float() # casting to float32 to match dtype of Weight\n",
    "xenc_train = xenc_train.view(xenc_train.shape[0], 2*27) \n",
    "yenc_train = F.one_hot(ys_train, num_classes=27).float()\n",
    "\n",
    "num_train = ys_train.nelement()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "220f8f93",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "g = torch.Generator().manual_seed(2147483647)\n",
    "W_tri = torch.randn(27*2, 27, generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8e44e072",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.368177652359009\n",
      "2.3670051097869873\n",
      "2.365851879119873\n",
      "2.364717483520508\n",
      "2.363600730895996\n",
      "2.362502098083496\n",
      "2.3614211082458496\n",
      "2.360356569290161\n",
      "2.3593087196350098\n",
      "2.3582775592803955\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    # Forward pass\n",
    "    logits = (xenc_train @ W_tri)\n",
    "    counts = logits.exp()\n",
    "    # Normalize the count to get probability distribution\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "\n",
    "    # print(torch.arange(num))\n",
    "    # print(ys.shape)\n",
    "    # print(logits.shape, probs.shape)\n",
    "    loss = -probs[torch.arange(num_train), ys_train].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # Backward pass\n",
    "    W_tri.grad = None\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights\n",
    "    W_tri.data += -10 * W_tri.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f792b579",
   "metadata": {},
   "source": [
    "### Trigram: loss on dev and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82bf2b83",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "xs_dev_tri, xs_test_tri, ys_dev_tri, ys_test_tri = [], [], [], []\n",
    "for name in dev_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs_dev_tri.append([stoi[ch1], stoi[ch2]])\n",
    "        ys_dev_tri.append(stoi[ch3])\n",
    "\n",
    "for name in test_list:\n",
    "    name = ['.'] + list(name) + ['.']\n",
    "    for ch1, ch2, ch3 in zip(name, name[1:], name[2:]):\n",
    "        xs_test_tri.append([stoi[ch1], stoi[ch2]])\n",
    "        ys_test_tri.append(stoi[ch3])\n",
    "\n",
    "xs_dev_tri = torch.tensor(xs_dev_tri)\n",
    "ys_dev_tri = torch.tensor(ys_dev_tri)\n",
    "xs_test_tri = torch.tensor(xs_test_tri)\n",
    "ys_test_tri = torch.tensor(ys_test_tri)\n",
    "\n",
    "num_dev_tri = len(ys_dev_tri)\n",
    "num_test_tri = len(ys_test_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09c6743d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "def compute_trigram_loss(xs, ys, W):\n",
    "    xenc = F.one_hot(xs, num_classes=27).float()\n",
    "    xenc = xenc.view(xenc.shape[0], 2*27) \n",
    "    logits = xenc @ W\n",
    "    counts = logits.exp()\n",
    "    probs = counts / counts.sum(1, keepdims=True)\n",
    "    loss = -probs[torch.arange(len(xs)), ys].log().mean()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "99615845",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "dev_loss_tri = compute_trigram_loss(xs_dev_tri, ys_dev_tri, W_tri)\n",
    "test_loss_tri = compute_trigram_loss(xs_test_tri, ys_test_tri, W_tri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "eea507b0",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dev loss on trigrams: 2.366079807281494\n",
      "Test loss on trigrams: 2.3610594272613525\n"
     ]
    }
   ],
   "source": [
    "print('Dev loss on trigrams:', dev_loss_tri.item())\n",
    "print('Test loss on trigrams:', test_loss_tri.item())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
