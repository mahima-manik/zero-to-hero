{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "05d44990",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "c0c377b1",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total names: 32033\n",
      "['emma', 'olivia', 'ava', 'isabella', 'sophia']\n"
     ]
    }
   ],
   "source": [
    "with open('../names.txt', 'r') as file:\n",
    "    names = file.read().split()\n",
    "\n",
    "print('Total names:', len(names))\n",
    "print(names[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0b230fb6",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "# Form stoi and itos\n",
    "vocab = sorted(list(set(''.join(names))))\n",
    "stoi = {s: i+1 for i, s in enumerate(vocab)}\n",
    "stoi['.'] = 0\n",
    "itos = {stoi[s]: s for s in stoi}\n",
    "\n",
    "vocab_size = len(stoi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f29e2d2",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "block_size = 3\n",
    "def form_dataset(words):\n",
    "    X = []\n",
    "    Y = []\n",
    "\n",
    "    for word in words:\n",
    "        word = ['.'] * block_size + list(word) + ['.']\n",
    "        for ind in range(3, len(word)):\n",
    "            X.append([stoi[x] for x in word[ind-3:ind]])\n",
    "            Y.append(stoi[word[ind]])\n",
    "            # print(''.join(word[ind-3:ind]), '--->', word[ind])\n",
    "\n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7b472f5a",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(names)\n",
    "\n",
    "X, Y = form_dataset(names)\n",
    "\n",
    "n1 = int(0.8 * X.shape[0])\n",
    "n2 = int(0.9 * X.shape[0])\n",
    "Xtr, Xdev, Xts = X.tensor_split((n1, n2), dim=0) # input is split into X[:n1], X[n1:n2] and X[n2:]\n",
    "Ytr, Ydev, Yts = Y.tensor_split((n1, n2), dim=0)\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "2401fd36",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        self.W = torch.randn((in_features, out_features), generator=g) / (in_features ** 2)\n",
    "        if bias:\n",
    "            self.b = torch.randn(out_features) if bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # forward pass\n",
    "        x_new = x.view(-1, self.W.shape[0])\n",
    "        self.out = x.view(-1, self.W.shape[0]) @ self.W\n",
    "        if self.b is not None:\n",
    "            self.out = self.out + self.b\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.W] + ([] if self.b is None else [self.b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "b1cbac76",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.0015,  0.0852, -0.0688,  0.0198,  0.1233,  0.0219],\n",
       "         [-0.0681,  0.1082, -0.0864,  0.0247,  0.0639,  0.0195],\n",
       "         [ 0.0523,  0.0265, -0.0279, -0.0180, -0.0002, -0.0081],\n",
       "         [-0.0108,  0.0295, -0.0216,  0.0384, -0.0032, -0.0268]]),\n",
       " tensor([-0.3411, -0.1157, -1.0170,  0.6771,  0.6893, -0.7846])]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer = Layer(4, 6)\n",
    "layer(torch.randn(100, 4))\n",
    "layer.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7ea2ee73",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def __call__(self, x):\n",
    "        self.out = torch.tanh(x)\n",
    "        return self.out\n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "fca43c0d",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3420,  0.3962,  0.0993, -0.4178,  0.7253, -0.8575, -0.9274, -0.9860,\n",
       "        -0.4772, -0.8924])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "th = Tanh()\n",
    "th(torch.randn(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "909773ca",
   "metadata": {
    "metadata": {}
   },
   "outputs": [],
   "source": [
    "class BatchNorm1d:\n",
    "    def __init__(self, dim, eps=1e-05, momentum=0.1):\n",
    "        \"\"\"\n",
    "        dim: Dimensionality or num_features\n",
    "        \"\"\"\n",
    "        self.dim = dim\n",
    "        self.eps = eps\n",
    "        self.momentum = momentum\n",
    "        self.training = True # By default for training\n",
    "\n",
    "        self.gamma = torch.ones(dim) # initialized to 1 for multiplication\n",
    "        self.beta = torch.zeros(dim) # initialized to 0 for addition\n",
    "\n",
    "        # track running stats is True by default\n",
    "        self.running_mean = torch.zeros(dim)\n",
    "        self.running_variance = torch.ones(dim)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            xmean = x.mean(0, keepdim=True) # batch mean\n",
    "            xstd = x.std(0, keepdim=True) # batch variance\n",
    "        else:\n",
    "            xmean = self.running_mean\n",
    "            xstd = self.running_variance\n",
    "\n",
    "        xhat = (x - xmean) / torch.sqrt(xstd + self.eps)\n",
    "        self.out =  self.gamma * xhat + self.beta\n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = self.momentum * xmean + (1-self.momentum) * self.running_mean\n",
    "                self.running_variance = self.momentum * xstd + (1-self.momentum) * self.running_variance\n",
    "        \n",
    "        return self.out\n",
    "\n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "20b5eb9e",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "fbc2d125",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 46227\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 # the dimensionality of the character embedding vectors\n",
    "n_hidden = 100 # the number of neurons in the hidden layer of the MLP\n",
    "\n",
    "C = torch.randn(vocab_size, n_embd) # used to encode the dataset into vector space of 10\n",
    "\n",
    "layers = [\n",
    "    Layer(n_embd * block_size, n_hidden), Tanh(),\n",
    "    Layer(n_hidden, n_hidden), Tanh(),\n",
    "    Layer(n_hidden, n_hidden), Tanh(),\n",
    "    Layer(n_hidden, n_hidden), Tanh(),\n",
    "    Layer(n_hidden, n_hidden), Tanh(),\n",
    "    Layer(n_hidden, vocab_size)\n",
    "]\n",
    "\n",
    "with torch.no_grad():\n",
    "    layers[-1].W = layers[-1].W * 0.1 # So that it is closer to 0 at the last layer and less confident in predictions\n",
    "\n",
    "parameters = [p for layer in layers for p in layer.parameters()]\n",
    "count = sum(p.nelement() for p in parameters)\n",
    "print('Total parameters:', count)\n",
    "for p in parameters:\n",
    "  p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "5c3a4229",
   "metadata": {
    "metadata": {}
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182516, 27])\n",
      "2.8959527015686035\n"
     ]
    }
   ],
   "source": [
    "embs = C[Xtr]\n",
    "for layer in layers:\n",
    "    embs = layer(embs)\n",
    "\n",
    "print(embs.shape)\n",
    "\n",
    "loss = F.cross_entropy(embs, Ytr)\n",
    "print(loss.item())\n",
    "\n",
    "for layer in layers:\n",
    "    for p in layer.parameters():\n",
    "        p.grad = None\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for layer in layers:\n",
    "    for p in layer.parameters():\n",
    "        p.data += -0.1 * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57533692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7c4423",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
